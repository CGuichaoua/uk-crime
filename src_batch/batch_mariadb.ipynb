{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion à MariaDB\n",
    "def create_connection(host_name, user_name, user_password, db_name=None, port=3306):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=host_name,\n",
    "            user=user_name,\n",
    "            password=user_password,\n",
    "            database=db_name,\n",
    "            port=port,\n",
    "            connection_timeout=600  # 10 minutes de délai\n",
    "        )\n",
    "        print(f\"Connexion réussie à la base de données {db_name}\" if db_name else \"Connexion réussie au serveur MariaDB\")\n",
    "        return connection\n",
    "    except Error as e:\n",
    "        print(f\"Erreur : '{e}'\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creer_table_si_absente(connection, nom_table, colonnes, dtype):\n",
    "    \"\"\"\n",
    "    Crée une table dans MariaDB si elle n'existe pas déjà,\n",
    "    en utilisant les colonnes extraites et leurs types.\n",
    "    \"\"\"\n",
    "    colonnes_avec_types = []\n",
    "    \n",
    "    for colonne in colonnes:\n",
    "        # On récupère le type depuis dtype ou on met 'VARCHAR(255)' par défaut\n",
    "        type_colonne = dtype.get(colonne, 'VARCHAR(255)')\n",
    "        \n",
    "        # Ajustement des types pour MariaDB\n",
    "        if type_colonne == 'float':\n",
    "            type_colonne = 'FLOAT'\n",
    "        elif type_colonne == 'int':\n",
    "            type_colonne = 'INT'\n",
    "        elif type_colonne == 'bool':\n",
    "            type_colonne = 'TINYINT(1)'\n",
    "        elif type_colonne == 'datetime':\n",
    "            type_colonne = 'DATETIME'\n",
    "    \n",
    "        \n",
    "        colonnes_avec_types.append(f\"{colonne} {type_colonne}\")\n",
    "    \n",
    "    # Génère la requête SQL avec les colonnes et leurs types\n",
    "    colonnes_avec_types_str = ', '.join(colonnes_avec_types)\n",
    "    \n",
    "    # Requête pour créer la table si elle n'existe pas déjà\n",
    "    requete_creation = f\"CREATE TABLE IF NOT EXISTS {nom_table} ({colonnes_avec_types_str});\"\n",
    "    \n",
    "    print(requete_creation)  # Pour déboguer, afficher la requête générée\n",
    "    \n",
    "    cursor=connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(requete_creation)  # Exécuter la requête\n",
    "        #connection.commit()\n",
    "        print(f\"Table '{nom_table}' créée avec succès (si absente)\")\n",
    "    except Error as e:\n",
    "        print(f\"Erreur lors de la création de la table : '{e}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inserer_donnees(connection, nom_table, colonnes, donnees):\n",
    "    \"\"\"\n",
    "    Insère les données dans la table MariaDB.\n",
    "    \"\"\"\n",
    "    # Création des placeholders pour MariaDB (%s)\n",
    "    placeholders = ', '.join(['%s' for _ in colonnes])\n",
    "    # Génère une chaîne avec les noms de colonnes\n",
    "    colonnes_str = ', '.join(colonnes)\n",
    "    # Création de la requête d'insertion\n",
    "    requete_insertion = f\"INSERT INTO {nom_table} ({colonnes_str}) VALUES ({placeholders});\"\n",
    "    cursor=connection.cursor()\n",
    "    try:\n",
    "        # Exécution de l'insertion avec executemany\n",
    "        cursor.executemany(requete_insertion, donnees)\n",
    "        # Commit pour enregistrer les changements dans la base de données\n",
    "        #connection.commit()\n",
    "        print(f\"{cursor.rowcount} lignes insérées avec succès dans la table {nom_table}\")\n",
    "    except Error as e:\n",
    "        print(f\"Erreur lors de l'insertion : '{e}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraire_info_du_nom_fichier(fichier,liste_nom_table):  \n",
    "    # extraction informations du nom de fichier à encoder dans la table\n",
    "    annee_mois=fichier[:7]\n",
    "    info_geo=fichier[8:-4]\n",
    "    # where sans l'élément qui est dans la liste\n",
    "    for i in liste_nom_table:\n",
    "        if i in info_geo:\n",
    "            info_geo=info_geo.replace(i,\"\")[:-1]\n",
    "    return annee_mois,info_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettoyer_noms_colonnes(colonnes):\n",
    "    \"\"\"\n",
    "    Nettoie les noms des colonnes pour éviter les problèmes de requêtes SQL.\n",
    "    \"\"\"\n",
    "     # nettoyage des noms de colonnes\n",
    "    colonnes = [colonne.replace(' ', '') for colonne in colonnes]\n",
    "    colonnes = [colonne.replace('-', '') for colonne in colonnes]\n",
    "    colonnes = [colonne.replace('_', '') for colonne in colonnes]\n",
    "    colonnes_nettoyees = [colonne.replace(' ', '').replace('-', '').replace('_', '') for colonne in colonnes]\n",
    "    return colonnes_nettoyees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettoyer_nom_table(liste_nom_table,chemin_fichier):\n",
    "    for nom_table in liste_nom_table:\n",
    "        if nom_table in chemin_fichier:\n",
    "            nom_table = nom_table.replace('-', '')\n",
    "            break\n",
    "        else:\n",
    "            nom_table = 'autre'\n",
    "            \n",
    "    print(f\"nom de la table : {nom_table}\")\n",
    "    # Créer la table si elle n'existe pas\n",
    "    return nom_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettoyer_donnees(donnees):\n",
    "    \"\"\"\n",
    "    Remplace les NaN dans les données par des valeurs par défaut.\n",
    "    Pour les chaînes, remplace par une chaîne vide, pour les numériques, par NULL.\n",
    "    \"\"\"\n",
    "\n",
    "    # extraire les doublons et supprimer les doublons - remplacer les nan\n",
    "    duplicates=donnees[donnees.duplicated()]\n",
    "    duplicates=duplicates.replace({np.nan:''})\n",
    "    \n",
    "    donnees=donnees.drop_duplicates()\n",
    "    donnees = donnees.replace({np.nan: ''})\n",
    "        \n",
    "    return donnees, duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rajouter_un_index_table(nom_table,index_dict,df):\n",
    "     # Ajouter une colonne d'index unique dans les tables\n",
    "    if nom_table not in index_dict:\n",
    "        index_dict[nom_table] = 1\n",
    "    df['id'] = range(index_dict[nom_table], index_dict[nom_table] + len(df))\n",
    "    index_dict[nom_table] += len(df)\n",
    "    return df,index_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traiter_fichier(connection,chemin_fichier, fichier,liste_nom_table,dtype, index_dict,liste_col_a_supprimer):\n",
    "    \"\"\"\n",
    "    Traite un fichier CSV en créant une table correspondante et en y insérant les données.\n",
    "    \"\"\"\n",
    "    # Gérer les valeurs manquantes\n",
    "    na_values = ['NA', 'N/A', '']\n",
    "        # Lire le fichier CSV\n",
    "    parse_dates = [col for col, typ in dtype.items() if typ == 'datetime']\n",
    "    df = pd.read_csv(chemin_fichier, dtype={col: typ for col, typ in dtype.items() if typ != 'datetime'},na_values=na_values)\n",
    "\n",
    "    # supprimer les colonnes de la liste 'liste_col_a_supprimer'\n",
    "    df = df.drop(columns=[col for col in liste_col_a_supprimer if col in df.columns])\n",
    " \n",
    "    # rajout des informations year_month et where dans le dataframe\n",
    "    annee_mois,info_geo = extraire_info_du_nom_fichier(fichier,liste_nom_table)\n",
    "    df['annee_mois'] = annee_mois\n",
    "    df['info_geo'] = info_geo\n",
    "\n",
    "    # nettoyer les données\n",
    "    df, duplicates = nettoyer_donnees(df)\n",
    "    duplicates['source']=fichier\n",
    "\n",
    "    nom_table = nettoyer_nom_table(liste_nom_table,chemin_fichier)+\"_temp\"\n",
    "    nom_table_duplicates=nom_table+\"_duplicates\"\n",
    "\n",
    "    # calculer l'index de table, insérer les données dans la table\n",
    "    df,index_dict=rajouter_un_index_table(nom_table,index_dict,df)\n",
    "    colonnes = nettoyer_noms_colonnes(list(df.columns))\n",
    "    creer_table_si_absente(connection,nom_table, colonnes,dtype)\n",
    "    inserer_donnees(connection,nom_table, colonnes, df.values.tolist())\n",
    "  \n",
    "    duplicates,index_dict=rajouter_un_index_table(nom_table_duplicates,index_dict,duplicates)\n",
    "    colonnes_duplicates=nettoyer_noms_colonnes(list(duplicates.columns))\n",
    "    creer_table_si_absente(connection,nom_table_duplicates, colonnes_duplicates,dtype)\n",
    "    inserer_donnees(connection,nom_table_duplicates, colonnes_duplicates, duplicates.values.tolist())\n",
    "  \n",
    "    print(f\"Traitement du fichier : {chemin_fichier} terminé.\")\n",
    "\n",
    "    return df, index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parcourir_arborescence(connection,chemin_racine, db_path,liste_nom_table,dtype,index_dict,liste_col_a_supprimer):\n",
    "    \"\"\"\n",
    "    Parcourt récursivement l'arborescence et traite chaque fichier CSV trouvé.\n",
    "    \"\"\"\n",
    "    \n",
    "    for racine, sous_repertoires, fichiers in os.walk(chemin_racine):\n",
    "         for fichier in fichiers:\n",
    "            if fichier.endswith(\".csv\"):\n",
    "                chemin_fichier = os.path.join(racine, fichier)\n",
    "                df, index_dict=traiter_fichier(connection,chemin_fichier, fichier,liste_nom_table,dtype, index_dict,liste_col_a_supprimer)\n",
    "    return df, index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion réussie à la base de données crime_test\n",
      "nom de la table : outcomes\n",
      "CREATE TABLE IF NOT EXISTS outcomes_temp (CrimeID VARCHAR(255), Month VARCHAR(255), Reportedby VARCHAR(255), Longitude FLOAT, Latitude FLOAT, Location VARCHAR(255), LSOAcode VARCHAR(255), LSOAname VARCHAR(255), Outcometype VARCHAR(255), anneemois VARCHAR(255), infogeo VARCHAR(255), id INT);\n",
      "Table 'outcomes_temp' créée avec succès (si absente)\n",
      "3050 lignes insérées avec succès dans la table outcomes_temp\n",
      "CREATE TABLE IF NOT EXISTS outcomes_temp_duplicates (CrimeID VARCHAR(255), Month VARCHAR(255), Reportedby VARCHAR(255), Longitude FLOAT, Latitude FLOAT, Location VARCHAR(255), LSOAcode VARCHAR(255), LSOAname VARCHAR(255), Outcometype VARCHAR(255), anneemois VARCHAR(255), infogeo VARCHAR(255), source VARCHAR(255), id INT);\n",
      "Table 'outcomes_temp_duplicates' créée avec succès (si absente)\n",
      "25 lignes insérées avec succès dans la table outcomes_temp_duplicates\n",
      "Traitement du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2019-11-bedfordshire-outcomes.csv terminé.\n",
      "nom de la table : stopandsearch\n",
      "CREATE TABLE IF NOT EXISTS stopandsearch_temp (Type VARCHAR(255), Date DATETIME, Partofapolicingoperation TINYINT(1), Policingoperation VARCHAR(255), Latitude FLOAT, Longitude FLOAT, Gender VARCHAR(255), Agerange VARCHAR(255), Selfdefinedethnicity VARCHAR(255), Officerdefinedethnicity VARCHAR(255), Legislation VARCHAR(255), Objectofsearch VARCHAR(255), Outcome VARCHAR(255), Outcomelinkedtoobjectofsearch TINYINT(1), Removalofmorethanjustouterclothing TINYINT(1), anneemois VARCHAR(255), infogeo VARCHAR(255), id INT);\n",
      "Table 'stopandsearch_temp' créée avec succès (si absente)\n",
      "242 lignes insérées avec succès dans la table stopandsearch_temp\n",
      "CREATE TABLE IF NOT EXISTS stopandsearch_temp_duplicates (Type VARCHAR(255), Date DATETIME, Partofapolicingoperation TINYINT(1), Policingoperation VARCHAR(255), Latitude FLOAT, Longitude FLOAT, Gender VARCHAR(255), Agerange VARCHAR(255), Selfdefinedethnicity VARCHAR(255), Officerdefinedethnicity VARCHAR(255), Legislation VARCHAR(255), Objectofsearch VARCHAR(255), Outcome VARCHAR(255), Outcomelinkedtoobjectofsearch TINYINT(1), Removalofmorethanjustouterclothing TINYINT(1), anneemois VARCHAR(255), infogeo VARCHAR(255), source VARCHAR(255), id INT);\n",
      "Table 'stopandsearch_temp_duplicates' créée avec succès (si absente)\n",
      "11 lignes insérées avec succès dans la table stopandsearch_temp_duplicates\n",
      "Traitement du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2019-11-bedfordshire-stop-and-search.csv terminé.\n",
      "nom de la table : street\n",
      "CREATE TABLE IF NOT EXISTS street_temp (CrimeID VARCHAR(255), Month VARCHAR(255), Reportedby VARCHAR(255), Longitude FLOAT, Latitude FLOAT, Location VARCHAR(255), LSOAcode VARCHAR(255), LSOAname VARCHAR(255), Crimetype VARCHAR(255), Lastoutcomecategory VARCHAR(255), Context VARCHAR(255), anneemois VARCHAR(255), infogeo VARCHAR(255), id INT);\n",
      "Table 'street_temp' créée avec succès (si absente)\n",
      "5258 lignes insérées avec succès dans la table street_temp\n",
      "CREATE TABLE IF NOT EXISTS street_temp_duplicates (CrimeID VARCHAR(255), Month VARCHAR(255), Reportedby VARCHAR(255), Longitude FLOAT, Latitude FLOAT, Location VARCHAR(255), LSOAcode VARCHAR(255), LSOAname VARCHAR(255), Crimetype VARCHAR(255), Lastoutcomecategory VARCHAR(255), Context VARCHAR(255), anneemois VARCHAR(255), infogeo VARCHAR(255), source VARCHAR(255), id INT);\n",
      "Table 'street_temp_duplicates' créée avec succès (si absente)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin.local\\AppData\\Local\\Temp\\ipykernel_6112\\1416925358.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  duplicates=duplicates.replace({np.nan:''})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 lignes insérées avec succès dans la table street_temp_duplicates\n",
      "Traitement du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2019-11-bedfordshire-street.csv terminé.\n",
      "nom de la table : outcomes\n",
      "CREATE TABLE IF NOT EXISTS outcomes_temp (CrimeID VARCHAR(255), Month VARCHAR(255), Reportedby VARCHAR(255), Longitude FLOAT, Latitude FLOAT, Location VARCHAR(255), LSOAcode VARCHAR(255), LSOAname VARCHAR(255), Outcometype VARCHAR(255), anneemois VARCHAR(255), infogeo VARCHAR(255), id INT);\n",
      "Table 'outcomes_temp' créée avec succès (si absente)\n",
      "6890 lignes insérées avec succès dans la table outcomes_temp\n",
      "CREATE TABLE IF NOT EXISTS outcomes_temp_duplicates (CrimeID VARCHAR(255), Month VARCHAR(255), Reportedby VARCHAR(255), Longitude FLOAT, Latitude FLOAT, Location VARCHAR(255), LSOAcode VARCHAR(255), LSOAname VARCHAR(255), Outcometype VARCHAR(255), anneemois VARCHAR(255), infogeo VARCHAR(255), source VARCHAR(255), id INT);\n",
      "Table 'outcomes_temp_duplicates' créée avec succès (si absente)\n",
      "482 lignes insérées avec succès dans la table outcomes_temp_duplicates\n",
      "Traitement du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2021-10-avon-and-somerset-outcomes.csv terminé.\n",
      "nom de la table : stopandsearch\n",
      "CREATE TABLE IF NOT EXISTS stopandsearch_temp (Type VARCHAR(255), Date DATETIME, Partofapolicingoperation TINYINT(1), Policingoperation VARCHAR(255), Latitude FLOAT, Longitude FLOAT, Gender VARCHAR(255), Agerange VARCHAR(255), Selfdefinedethnicity VARCHAR(255), Officerdefinedethnicity VARCHAR(255), Legislation VARCHAR(255), Objectofsearch VARCHAR(255), Outcome VARCHAR(255), Outcomelinkedtoobjectofsearch TINYINT(1), Removalofmorethanjustouterclothing TINYINT(1), anneemois VARCHAR(255), infogeo VARCHAR(255), id INT);\n",
      "Table 'stopandsearch_temp' créée avec succès (si absente)\n",
      "642 lignes insérées avec succès dans la table stopandsearch_temp\n",
      "CREATE TABLE IF NOT EXISTS stopandsearch_temp_duplicates (Type VARCHAR(255), Date DATETIME, Partofapolicingoperation TINYINT(1), Policingoperation VARCHAR(255), Latitude FLOAT, Longitude FLOAT, Gender VARCHAR(255), Agerange VARCHAR(255), Selfdefinedethnicity VARCHAR(255), Officerdefinedethnicity VARCHAR(255), Legislation VARCHAR(255), Objectofsearch VARCHAR(255), Outcome VARCHAR(255), Outcomelinkedtoobjectofsearch TINYINT(1), Removalofmorethanjustouterclothing TINYINT(1), anneemois VARCHAR(255), infogeo VARCHAR(255), source VARCHAR(255), id INT);\n",
      "Table 'stopandsearch_temp_duplicates' créée avec succès (si absente)\n",
      "36 lignes insérées avec succès dans la table stopandsearch_temp_duplicates\n",
      "Traitement du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2021-10-avon-and-somerset-stop-and-search.csv terminé.\n",
      "nom de la table : street\n",
      "CREATE TABLE IF NOT EXISTS street_temp (CrimeID VARCHAR(255), Month VARCHAR(255), Reportedby VARCHAR(255), Longitude FLOAT, Latitude FLOAT, Location VARCHAR(255), LSOAcode VARCHAR(255), LSOAname VARCHAR(255), Crimetype VARCHAR(255), Lastoutcomecategory VARCHAR(255), Context VARCHAR(255), anneemois VARCHAR(255), infogeo VARCHAR(255), id INT);\n",
      "Table 'street_temp' créée avec succès (si absente)\n",
      "13820 lignes insérées avec succès dans la table street_temp\n",
      "CREATE TABLE IF NOT EXISTS street_temp_duplicates (CrimeID VARCHAR(255), Month VARCHAR(255), Reportedby VARCHAR(255), Longitude FLOAT, Latitude FLOAT, Location VARCHAR(255), LSOAcode VARCHAR(255), LSOAname VARCHAR(255), Crimetype VARCHAR(255), Lastoutcomecategory VARCHAR(255), Context VARCHAR(255), anneemois VARCHAR(255), infogeo VARCHAR(255), source VARCHAR(255), id INT);\n",
      "Table 'street_temp_duplicates' créée avec succès (si absente)\n",
      "734 lignes insérées avec succès dans la table street_temp_duplicates\n",
      "Traitement du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2021-10-avon-and-somerset-street.csv terminé.\n",
      "Connexion MariaDB fermée\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chemin_racine = \"C:/Users/Admin.local/Documents/projetint/files_test\"\n",
    "#chemin_racine = \"C:/Users/Admin.local/Documents/projetint/files\"\n",
    "\n",
    "db_name = \"crime_test\" \n",
    "\n",
    "# on définit le nom des tables en fonction du nom du fichier (terminaison)\n",
    "liste_nom_table = ['outcomes','stop-and-search','street']\n",
    "\n",
    "# encodage des types en fonction de la colonne\n",
    "dtype={'Longitude': 'float',\n",
    "        'Latitude': 'float',\n",
    "        'id':'int',\n",
    "        'Partofapolicingoperation': 'bool',\n",
    "        'Date': 'datetime',\n",
    "        'Outcomelinkedtoobjectofsearch': 'bool',\n",
    "        'Removalofmorethanjustouterclothing': 'bool'\n",
    "}\n",
    "\n",
    "liste_col_a_supprimer=['Falls within']\n",
    "\n",
    "# Dictionnaire pour mémoriser l'index pour chaque table\n",
    "index_dict = {}\n",
    "\n",
    "# Connexion à la base de données 'crime'\n",
    "connection = create_connection(\"127.0.0.1\", \"root\", \"\", db_name)\n",
    "\n",
    "if connection:\n",
    "    df, index_dict=parcourir_arborescence(connection,chemin_racine, db_name,liste_nom_table,dtype, index_dict,liste_col_a_supprimer)\n",
    "    # Fermeture de la connexion\n",
    "    if connection.is_connected():\n",
    "        connection.commit()\n",
    "        connection.close()\n",
    "        print(\"Connexion MariaDB fermée\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outcomes_temp': 9941, 'outcomes_temp_duplicates': 508, 'stopandsearch_temp': 885, 'stopandsearch_temp_duplicates': 48, 'street_temp': 19079, 'street_temp_duplicates': 953}\n"
     ]
    }
   ],
   "source": [
    "print(index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
