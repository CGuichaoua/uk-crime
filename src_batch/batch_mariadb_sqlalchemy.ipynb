{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, Float, Boolean, DateTime\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connexion à MariaDB via SQLAlchemy\n",
    "def create_connection_sqlalchemy(host_name, user_name, user_password, db_name=None, port=3306):\n",
    "    try:\n",
    "        db_url = f\"mysql+pymysql://{user_name}:{user_password}@{host_name}:{port}/{db_name}\"\n",
    "        engine = create_engine(db_url, pool_recycle=3600)\n",
    "        connection = engine.connect()\n",
    "        print(f\"Connexion réussie à la base de données {db_name}\" if db_name else \"Connexion réussie au serveur MariaDB\")\n",
    "        return connection, engine\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Erreur : '{e}'\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def creer_table_si_absente(connection, engine, nom_table, colonnes, dtype):\n",
    "    \"\"\"\n",
    "    Crée une table si elle n'existe pas déjà dans la base de données.\n",
    "    \"\"\"\n",
    "    metadata = MetaData()\n",
    "    colonnes_avec_types = []\n",
    "\n",
    "    for colonne in colonnes:\n",
    "        col_type = dtype.get(colonne, String(255))\n",
    "        if col_type == 'int':\n",
    "            col_type = Integer\n",
    "        elif col_type == 'float':\n",
    "            col_type = Float\n",
    "        elif col_type == 'bool':\n",
    "            col_type = Boolean\n",
    "        elif col_type == 'datetime':\n",
    "            col_type = DateTime\n",
    "        else:\n",
    "            col_type = String(255)\n",
    "        colonnes_avec_types.append(Column(colonne, col_type))\n",
    "\n",
    "    table = Table(nom_table, metadata, *colonnes_avec_types)\n",
    "    metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inserer_donnees(connection, nom_table, colonnes, donnees):\n",
    "    \"\"\"\n",
    "    Insère les données dans la table MariaDB via SQLAlchemy.\n",
    "    \"\"\"\n",
    "    metadata = MetaData()\n",
    "    table = Table(nom_table, metadata, autoload_with=connection)\n",
    "    \n",
    "    # Convertir les données en une liste de dictionnaires\n",
    "    data_dicts = [dict(zip(colonnes, row)) for row in donnees]\n",
    "    \n",
    "    try:\n",
    "        connection.execute(table.insert(), data_dicts)\n",
    "        connection.commit()\n",
    "        print(f\"{len(donnees)} lignes insérées avec succès dans la table {nom_table}\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Erreur lors de l'insertion : '{e}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extraire_info_du_nom_fichier(fichier,liste_nom_table):  \n",
    "    # extraction informations du nom de fichier à encoder dans la table\n",
    "    annee_mois=fichier[:7]\n",
    "    info_geo=fichier[8:-4]\n",
    "    # where sans l'élément qui est dans la liste\n",
    "    for i in liste_nom_table:\n",
    "        if i in info_geo:\n",
    "            info_geo=info_geo.replace(i,\"\")[:-1]\n",
    "    return annee_mois,info_geo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nettoyer_noms_colonnes(colonnes):\n",
    "    \"\"\"\n",
    "    Nettoie les noms des colonnes pour éviter les problèmes de requêtes SQL.\n",
    "    \"\"\"\n",
    "     # nettoyage des noms de colonnes\n",
    "    colonnes = [colonne.replace(' ', '') for colonne in colonnes]\n",
    "    colonnes = [colonne.replace('-', '') for colonne in colonnes]\n",
    "    colonnes = [colonne.replace('_', '') for colonne in colonnes]\n",
    "    colonnes_nettoyees = [colonne.replace(' ', '').replace('-', '').replace('_', '') for colonne in colonnes]\n",
    "    return colonnes_nettoyees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nettoyer_nom_table(liste_nom_table,chemin_fichier):\n",
    "    for nom_table in liste_nom_table:\n",
    "        if nom_table in chemin_fichier:\n",
    "            nom_table = nom_table.replace('-', '')\n",
    "            break\n",
    "        else:\n",
    "            nom_table = 'autre'\n",
    "            \n",
    "    print(f\"nom de la table : {nom_table}\")\n",
    "    # Créer la table si elle n'existe pas\n",
    "    return nom_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettoyer_donnees(donnees):\n",
    "    \"\"\"\n",
    "    Remplace les NaN dans les données par des valeurs par défaut.\n",
    "    Pour les chaînes, remplace par une chaîne vide, pour les numériques, par NULL.\n",
    "    \"\"\"\n",
    "\n",
    "    # extraire les doublons et supprimer les doublons - remplacer les nan\n",
    "    duplicates=donnees[donnees.duplicated()]\n",
    "    duplicates=duplicates.replace({np.nan:''})\n",
    "    \n",
    "    donnees=donnees.drop_duplicates()\n",
    "    donnees = donnees.replace({np.nan: ''})\n",
    "        \n",
    "    return donnees, duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rajouter_un_index_table(nom_table,index_dict,df):\n",
    "     # Ajouter une colonne d'index unique dans les tables\n",
    "    if nom_table not in index_dict:\n",
    "        index_dict[nom_table] = 1\n",
    "    df['id'] = range(index_dict[nom_table], index_dict[nom_table] + len(df))\n",
    "    index_dict[nom_table] += len(df)\n",
    "    return df,index_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def traiter_fichier(connection, engine, chemin_fichier, fichier, liste_nom_table, dtype, index_dict, liste_col_a_supprimer):\n",
    "    \"\"\"\n",
    "    Traite un fichier CSV en créant une table correspondante et en y insérant les données.\n",
    "    \"\"\"\n",
    "    na_values = ['NA', 'N/A', '']\n",
    "    df = pd.read_csv(chemin_fichier, dtype={col: typ for col, typ in dtype.items() if typ != 'datetime'}, na_values=na_values)\n",
    "\n",
    "    # Supprimer les colonnes de la liste 'liste_col_a_supprimer'\n",
    "    df = df.drop(columns=[col for col in liste_col_a_supprimer if col in df.columns])\n",
    "\n",
    "    # Extraire et ajouter des informations au dataframe\n",
    "    annee_mois, info_geo = extraire_info_du_nom_fichier(fichier, liste_nom_table)\n",
    "    df['annee_mois'] = annee_mois\n",
    "    df['info_geo'] = info_geo\n",
    "\n",
    "    # Nettoyer les données\n",
    "    df, duplicates = nettoyer_donnees(df)\n",
    "    duplicates['source'] = fichier\n",
    "\n",
    "    nom_table = nettoyer_nom_table(liste_nom_table, chemin_fichier) + \"_temp\"\n",
    "    nom_table_duplicates = nom_table + \"_duplicates\"\n",
    "\n",
    "    # Ajouter un index unique\n",
    "    df, index_dict = rajouter_un_index_table(nom_table, index_dict, df)\n",
    "    colonnes = nettoyer_noms_colonnes(list(df.columns))\n",
    "    creer_table_si_absente(connection, engine, nom_table, colonnes, dtype)\n",
    "    inserer_donnees(connection, nom_table, colonnes, df.values.tolist())\n",
    "\n",
    "    # Traiter les doublons\n",
    "    duplicates, index_dict = rajouter_un_index_table(nom_table_duplicates, index_dict, duplicates)\n",
    "    colonnes_duplicates = nettoyer_noms_colonnes(list(duplicates.columns))\n",
    "    creer_table_si_absente(connection, engine, nom_table_duplicates, colonnes_duplicates, dtype)\n",
    "    inserer_donnees(connection, nom_table_duplicates, colonnes_duplicates, duplicates.values.tolist())\n",
    "\n",
    "    print(f\"Traitement du fichier : {chemin_fichier} terminé.\")\n",
    "    return df, index_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parcourir_arborescence(connection,engine,chemin_racine, db_path,liste_nom_table,dtype,index_dict,liste_col_a_supprimer):\n",
    "    \"\"\"\n",
    "    Parcourt récursivement l'arborescence et traite chaque fichier CSV trouvé.\n",
    "    \"\"\"\n",
    "    for racine, sous_repertoires, fichiers in os.walk(chemin_racine):\n",
    "        print(f\"racine : {racine}\")\n",
    "        print(f\"sous-repertoires : {sous_repertoires}\")\n",
    "        print(f\"fichiers : {fichiers}\")\n",
    "\n",
    "        for fichier in fichiers:\n",
    "            if fichier.endswith(\".csv\"):\n",
    "                chemin_fichier = os.path.join(racine, fichier)\n",
    "                print(f\"chemin du fichier : {chemin_fichier}\")\n",
    "                df, index_dict=traiter_fichier(connection,engine,chemin_fichier, fichier,liste_nom_table,dtype, index_dict,liste_col_a_supprimer)\n",
    "                \n",
    "    return df, index_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion réussie à la base de données crime_short\n",
      "racine : C:/Users/Admin.local/Documents/projetint/files_test\n",
      "sous-repertoires : ['files']\n",
      "fichiers : []\n",
      "racine : C:/Users/Admin.local/Documents/projetint/files_test\\files\n",
      "sous-repertoires : []\n",
      "fichiers : ['2019-11-bedfordshire-outcomes.csv', '2019-11-bedfordshire-stop-and-search.csv', '2019-11-bedfordshire-street.csv', '2021-10-avon-and-somerset-outcomes.csv', '2021-10-avon-and-somerset-stop-and-search.csv', '2021-10-avon-and-somerset-street.csv']\n",
      "chemin du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2019-11-bedfordshire-outcomes.csv\n",
      "nom de la table : outcomes\n",
      "3050 lignes insérées avec succès dans la table outcomes_temp\n",
      "25 lignes insérées avec succès dans la table outcomes_temp_duplicates\n",
      "Traitement du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2019-11-bedfordshire-outcomes.csv terminé.\n",
      "chemin du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2019-11-bedfordshire-stop-and-search.csv\n",
      "nom de la table : stopandsearch\n",
      "242 lignes insérées avec succès dans la table stopandsearch_temp\n",
      "11 lignes insérées avec succès dans la table stopandsearch_temp_duplicates\n",
      "Traitement du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2019-11-bedfordshire-stop-and-search.csv terminé.\n",
      "chemin du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2019-11-bedfordshire-street.csv\n",
      "nom de la table : street\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin.local\\AppData\\Local\\Temp\\ipykernel_18384\\1416925358.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  duplicates=duplicates.replace({np.nan:''})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5258 lignes insérées avec succès dans la table street_temp\n",
      "218 lignes insérées avec succès dans la table street_temp_duplicates\n",
      "Traitement du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2019-11-bedfordshire-street.csv terminé.\n",
      "chemin du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2021-10-avon-and-somerset-outcomes.csv\n",
      "nom de la table : outcomes\n",
      "6890 lignes insérées avec succès dans la table outcomes_temp\n",
      "482 lignes insérées avec succès dans la table outcomes_temp_duplicates\n",
      "Traitement du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2021-10-avon-and-somerset-outcomes.csv terminé.\n",
      "chemin du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2021-10-avon-and-somerset-stop-and-search.csv\n",
      "nom de la table : stopandsearch\n",
      "642 lignes insérées avec succès dans la table stopandsearch_temp\n",
      "36 lignes insérées avec succès dans la table stopandsearch_temp_duplicates\n",
      "Traitement du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2021-10-avon-and-somerset-stop-and-search.csv terminé.\n",
      "chemin du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2021-10-avon-and-somerset-street.csv\n",
      "nom de la table : street\n",
      "13820 lignes insérées avec succès dans la table street_temp\n",
      "734 lignes insérées avec succès dans la table street_temp_duplicates\n",
      "Traitement du fichier : C:/Users/Admin.local/Documents/projetint/files_test\\files\\2021-10-avon-and-somerset-street.csv terminé.\n",
      "Connexion fermée.\n",
      "{'outcomes_temp': 9941, 'outcomes_temp_duplicates': 508, 'stopandsearch_temp': 885, 'stopandsearch_temp_duplicates': 48, 'street_temp': 19079, 'street_temp_duplicates': 953}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chemin_racine = \"C:/Users/Admin.local/Documents/projetint/files_test\"\n",
    "\n",
    "db_name = \"crime_short\" \n",
    "\n",
    "# on définit le nom des tables en fonction du nom du fichier (terminaison)\n",
    "liste_nom_table = ['outcomes','stop-and-search','street']\n",
    "\n",
    "# encodage des types en fonction de la colonne\n",
    "dtype={'Longitude': 'float',\n",
    "        'Latitude': 'float',\n",
    "        'id':'int',\n",
    "        'Partofapolicingoperation': 'bool',\n",
    "        'Date': 'datetime',\n",
    "        'Outcomelinkedtoobjectofsearch': 'bool',\n",
    "        'Removalofmorethanjustouterclothing': 'bool'\n",
    "}\n",
    "\n",
    "liste_col_a_supprimer=['Falls within']\n",
    "\n",
    "# Dictionnaire pour mémoriser l'index pour chaque table\n",
    "index_dict = {}\n",
    "\n",
    "# Connexion à la base de données via SQLAlchemy\n",
    "connection, engine = create_connection_sqlalchemy(\"127.0.0.1\", \"root\", \"\",db_name)\n",
    "\n",
    "if connection:\n",
    "    df, index_dict = parcourir_arborescence(connection,engine, chemin_racine, db_name, liste_nom_table, dtype, index_dict, liste_col_a_supprimer)\n",
    "    \n",
    "    # Fermer la connexion proprement\n",
    "    connection.close()\n",
    "    print(\"Connexion fermée.\")\n",
    "  \n",
    "print(index_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
